{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to learning to rank with scikit-learn using the MLSR-WEB30k dataset.\n",
    "\n",
    "Code partly based on the blog post by Fabian Pedregosa:\n",
    "http://fa.bianp.net/blog/2012/learning-to-rank-with-scikit-learn-the-pairwise-transform/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pylab\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "# Set the random seed to be predictable.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO(brendan): relate the following to a real dataset of queries, rankings,\n",
    "and scores (e.g., clickthrough data). It is important to provide context for\n",
    "this toy example, or people will lose interest immediately.\n",
    "\n",
    "Create a dataset where target values consist of measurements Y = {0, 1, 2},\n",
    "and input data is 30 samples with two features each.\n",
    "\n",
    "Queries are generated from two normal distributions X1 and X2 of different\n",
    "means and covariances.\n",
    "\n",
    "Data from each of the two partitions follow vectors parallel to unit vector w,\n",
    "which is at angle theta to horizontal, with added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.deg2rad(60)\n",
    "w = np.array([np.sin(theta), np.cos(theta)])\n",
    "\n",
    "# The input data, in X, consist of two partitions of 3*K/2 points each. Each\n",
    "# input datum has two features.\n",
    "#\n",
    "# Each partition has three clusters of K/2 data points, one for each Y label,\n",
    "# where each cluster is normally distributed with mean proportional to the\n",
    "# cluster number along vector w.\n",
    "K = 20\n",
    "X = np.random.randn(K, 2)\n",
    "y = [0] * K\n",
    "for i in range(1, 3):\n",
    "    X = np.concatenate((X, np.random.randn(K, 2) + i*4*w))\n",
    "    y = np.concatenate((y, [i] * K))\n",
    "\n",
    "# Slightly displace data corresponding to our second partition, which is all\n",
    "# the even indices of X.\n",
    "part0_offset = np.array([-3, -7])\n",
    "X[::2] += part0_offset\n",
    "\n",
    "# Blocks refers to the partition indices, i.e., even indices of X belong to\n",
    "# block (partition) zero, and odd indices of X belong to block one.\n",
    "blocks = np.array([0, 1] * (X.shape[0] // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test set halves.\n",
    "\n",
    "`StratifiedShuffleSplit` splits the dataset into even strata, where each\n",
    "split retains class representations from the overall population, and\n",
    "cv.split() iterates over shuffled splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = sklearn.model_selection.StratifiedShuffleSplit(test_size=0.5)\n",
    "train, test = next(cv.split(X, y))\n",
    "X_train, y_train, b_train = X[train], y[train], blocks[train]\n",
    "X_test, y_test, b_test = X[test], y[test], blocks[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result, for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (b_train == 0)\n",
    "\n",
    "# Partition zero.\n",
    "pylab.scatter(X_train[idx, 0],\n",
    "              X_train[idx, 1],\n",
    "              c=y_train[idx],\n",
    "              marker='^',\n",
    "              cmap=pylab.cm.Blues,\n",
    "              s=100,\n",
    "              edgecolors='black')\n",
    "\n",
    "# w vector with partition zero's offset.\n",
    "pylab.arrow(part0_offset[0],\n",
    "            part0_offset[1],\n",
    "            8 * w[0],\n",
    "            8 * w[1],\n",
    "            fc='gray',\n",
    "            ec='gray',\n",
    "            head_width=0.5,\n",
    "            head_length=0.5)\n",
    "pylab.text(-2.6, -7, '$w$', fontsize=20)\n",
    "\n",
    "# Partition one.\n",
    "pylab.scatter(X_train[~idx, 0],\n",
    "              X_train[~idx, 1],\n",
    "              c=y_train[~idx],\n",
    "              marker='o',\n",
    "              cmap=pylab.cm.Blues,\n",
    "              s=100,\n",
    "              edgecolors='black')\n",
    "\n",
    "# w vector with partition one's offset.\n",
    "pylab.arrow(0,\n",
    "            0,\n",
    "            8 * w[0],\n",
    "            8 * w[1],\n",
    "            fc='gray',\n",
    "            ec='gray',\n",
    "            head_width=0.5,\n",
    "            head_length=0.5)\n",
    "pylab.text(0, 1, '$w$', fontsize=20)\n",
    "\n",
    "pylab.axis('equal')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the figure that there is a common vector w onto which the three\n",
    "clusters for each partition (or query) could be projected to give the correct\n",
    "ordering.\n",
    "\n",
    "Let's try to naively fit a single vector to the data via ridge regression, in\n",
    "order to demonstrate the need for query structure in our predictive modeling of\n",
    "search rankings. We will see that ridge regression tries to fit both queries at\n",
    "the same time, and therefore produces a poor fit.\n",
    "\n",
    "__Exercise__: Use scikit-learn to fit a ridge regression model to the data, and\n",
    "plot the result.\n",
    "\n",
    "_Hint_: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "Write your solution in the skeleton function definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rr(X_train, y_train, idx):\n",
    "    \"\"\"Fit dataset (X_train, y_train) using ridge regression, i.e., fit a\n",
    "    linear model with L2 weight regularization.\n",
    "\n",
    "    Args:\n",
    "        X_train: [N, 2] array of input features.\n",
    "        y_train: N length vector of labels in {0, 1, 2}, indicating each\n",
    "            datapoint's ordinal relevance score.\n",
    "        idx: N length array of boolean values, where True means that this\n",
    "            example belongs to query (block) 0, and False means query 1.\n",
    "\n",
    "    Return the fitted ridge regression model.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the code you wrote in fit_rr() to fit a ridge regression model, and\n",
    "plot the resulting fit along with our query ranking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this before giving the ridge regression exercise a shot. Set SOLN=1\n",
    "# only to cheat and use the solution.\n",
    "if os.getenv('SOLN') is not None:\n",
    "    from fit_rr import fit_rr\n",
    "\n",
    "ridge = fit_rr(X_train, y_train, idx)\n",
    "rr_coef = ridge.coef_ / np.linalg.norm(ridge.coef_)\n",
    "\n",
    "pylab.scatter(X_train[idx, 0],\n",
    "              X_train[idx, 1],\n",
    "              c=y_train[idx],\n",
    "              marker='^',\n",
    "              cmap=pylab.cm.Blues,\n",
    "              s=100,\n",
    "              edgecolors='black')\n",
    "pylab.scatter(X_train[~idx, 0],\n",
    "              X_train[~idx, 1],\n",
    "              c=y_train[~idx],\n",
    "              marker='o',\n",
    "              cmap=pylab.cm.Blues,\n",
    "              s=100,\n",
    "              edgecolors='black')\n",
    "pylab.arrow(0,\n",
    "            0,\n",
    "            7 * rr_coef[0],\n",
    "            7 * rr_coef[1],\n",
    "            fc='gray',\n",
    "            ec='gray',\n",
    "            head_width=0.5,\n",
    "            head_length=0.5)\n",
    "pylab.text(2, 0, '$\\hat{w}$', fontsize=20)\n",
    "pylab.axis('equal')\n",
    "pylab.title('Estimation by Ridge regression')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Kendall's tau coefficient on the test set to evaluate the\n",
    "quality of the ridge regression fit with respect to the true orderings in\n",
    "queries 0 and 1.\n",
    "\n",
    "Kendall's tau\n",
    "(https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) is a\n",
    "measure of rank correlation, i.e., a measure of similarity between two\n",
    "orderings of the same data, and takes all pairwise combinations of the data as\n",
    "input, returning a real valued output between -1 and 1.\n",
    "\n",
    "Define concordant pairs as all of the pairs for which the orderings are in\n",
    "agreement, define discordant pairs as all pairs that the orderings disagree on,\n",
    "and assume there are n data points. Then Kendall's tau is:\n",
    "\n",
    "tau = (# concordant pairs - # discordant pairs)/(n choose 2)\n",
    "\n",
    "__Exercise__: Using the test set and the fitted ridge regression model, write a\n",
    "function to compute and return Kendall's tau for a single query.\n",
    "\n",
    "_Hint_: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.kendalltau.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendalls_tau(ridge_model, X_query, y_query):\n",
    "    \"\"\"Compute and return Kendall's tau for X_query and y_query.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use your Kendall's tau function to evaluate the ridge regression fit\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, ignore this and first try the exercise.\n",
    "if os.getenv('SOLN') is not None:\n",
    "    from kendalls_tau import kendalls_tau\n",
    "\n",
    "for i in range(2):\n",
    "    tau = kendalls_tau(ridge, X_test[b_test == i], y_test[b_test == i])\n",
    "    print(f\"Kendall's tau coefficient for block {i}: {tau}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The pairwise transform\n",
    "\n",
    "(Herbrich, 1999) suggests that Kendall's tau, which counts inversions of pairs,\n",
    "can be based on a new training set whose elements are pairs (x1, x2), with x1\n",
    "and x2 from the original dataset. The label of element (x1, x2) in the new\n",
    "training set is -1 if x2 is preferred to x1, and +1 if x1 is preferred to x2\n",
    "(and zero if x1 and x2's ordinal score is equal). (Herbrich, 1999) shows that\n",
    "minimizing the 0-1 classification loss on the new pairs dataset is equivalent\n",
    "to minimizing Kendall's tau on the original dataset, up to a constant factor.\n",
    "\n",
    "__Exercise__: What is a potential pitfall of the pairwise transform, as defined\n",
    "above?\n",
    "\n",
    "We further transform the pairs (x1, x2) into (x1 - x2), such that the new\n",
    "dataset consists of points (x1 - x2, sign(y1 - y2)), where (x1, y1) and\n",
    "(x2, y2) are (feature, label) pairs from the original dataset. This transforms\n",
    "the original dataset into a binary classification problem with features of the\n",
    "same dimensionality as the original features.\n",
    "\n",
    "Note that since rankings only make sense with respect to the same query, only\n",
    "pairs from the same query group are included in the new dataset (and hence\n",
    "there is no exponential explosion of number of pairs).\n",
    "\n",
    "Let's form all pairwise combinations (for each query separately), and plot the\n",
    "new dataset formed by the pairwise differences for each query, and their\n",
    "ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form all combinations for which there is preference one way or another, and\n",
    "# both examples are from the same query.\n",
    "combinations = [(i, j)\n",
    "                for i in range(X_train.shape[0])\n",
    "                for j in range(X_train.shape[0])\n",
    "                if ((y_train[i] != y_train[j]) and\n",
    "                    (blocks[train][i] == blocks[train][j]))]\n",
    "\n",
    "Xp = np.array([X_train[i] - X_train[j] for i, j in combinations])\n",
    "diff = np.array([y_train[i] - y_train[j] for i, j in combinations])\n",
    "yp = np.array([np.sign(d) for d in diff])\n",
    "\n",
    "# Plot the dataset of differences (x_i - x_j) with labels sign(y_i - y_j), and\n",
    "# draw the hyperplane (line, in this 2D case) with the normal vector w, which\n",
    "# is the unit vector we defined at the start. This line separates the +1 class\n",
    "# (i is preferred to j) from the -1 class (j is preferred to i).\n",
    "pylab.scatter(Xp[:, 0],\n",
    "              Xp[:, 1],\n",
    "              c=diff,\n",
    "              s=60,\n",
    "              marker='o',\n",
    "              cmap=pylab.cm.Blues,\n",
    "              edgecolors='black')\n",
    "x_space = np.linspace(-10, 10)\n",
    "pylab.plot(x_space*w[1], -x_space*w[0], color='gray')\n",
    "pylab.text(3, -4, r'$\\{x^T w = 0\\}$', fontsize=17)\n",
    "pylab.axis('equal')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are linearly separable since in our generated dataset there were no\n",
    "inversions, i.e., pairs of data points that project onto w in the opposite\n",
    "order of their respective ranks. In general the data will not always be\n",
    "linearly separable.\n",
    "\n",
    "Let's train a RankSVM model on the dataset we have constructed from differences\n",
    "of pairs from the original dataset, i.e., Xp and yp.\n",
    "\n",
    "RankSVM\n",
    "[(Joachim, 2002)](http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
